{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact-Checking Engine\n",
    "\n",
    "### Test data path\n",
    "\n",
    "This constant holds the path to a test data file. Change this path accordingly. We recommend placing the test data file in the data folder within the unzipped directory and using a relative path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = \"data/fokg-sw-test-2024.nt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "Import the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py-310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "import pandas as pd\n",
    "from rdflib import Graph, URIRef, RDF, Namespace\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.models import TransE\n",
    "from pykeen.training import SLCWATrainingLoop\n",
    "from pykeen.losses import MarginRankingLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Load the reference, training, and test knowledge graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Knowledge Graph length. 675859\n",
      "Training data length. 5000\n",
      "Test data length. 2000\n"
     ]
    }
   ],
   "source": [
    "# Load reference knowledge graph\n",
    "reference_kg = Graph()\n",
    "reference_kg.parse(\"data/reference-kg.nt\", format=\"nt\")\n",
    "print(\"Reference Knowledge Graph length.\", len(reference_kg))\n",
    "\n",
    "# Load training data\n",
    "train_graph = Graph()\n",
    "train_graph.parse(\"data/fokg-sw-train-2024.nt\", format=\"nt\")\n",
    "print(\"Training data length.\", len(train_graph))\n",
    "\n",
    "#Load test data\n",
    "test_graph = Graph()\n",
    "test_graph.parse(TEST_DATA_PATH, format=\"nt\")\n",
    "print(\"Test data length.\", len(test_graph))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training TransE model with reference_kg\n",
    "\n",
    "Prepare the reference triples from the reference knowledge graph. Train the TransE model to learn embeddings for entities and relations in the reference knowledge graph, which can later be used for downstream tasks like link prediction or fact-checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of triples in reference_kg:  660000\n",
      "Training TransE for 10 epochs, batch_size=256 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs on cpu: 100%|██████████| 10/10 [18:13<00:00, 109.37s/epoch, loss=0.0145, prev_loss=0.0147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransE model trained with reference_kg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "reference_triples = [(str(subj), str(pred), str(obj)) for subj, pred, obj in reference_kg if isinstance(subj, rdflib.URIRef) and isinstance(obj, rdflib.URIRef)]\n",
    "print(\"Number of triples in reference_kg: \", len(reference_triples))\n",
    "reference_factory = TriplesFactory.from_labeled_triples(np.array(reference_triples, dtype=object))\n",
    "\n",
    "\n",
    "# TransE Model\n",
    "embedding_dim = 200\n",
    "margin = 1.0\n",
    "\n",
    "model = TransE(\n",
    "    triples_factory=reference_factory,\n",
    "    embedding_dim=embedding_dim,\n",
    "    scoring_fct_norm=1,  # L1 distance\n",
    "    loss=MarginRankingLoss(margin=margin),  # margin-based ranking\n",
    "    random_seed= 42,\n",
    ")\n",
    "\n",
    "training_kg_loop = SLCWATrainingLoop(\n",
    "    model=model,\n",
    "    triples_factory=reference_factory,\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_kwargs={\"lr\": 1e-3},\n",
    "    negative_sampler=\"basic\",\n",
    "    negative_sampler_kwargs={\"num_negs_per_pos\": 10},\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 256\n",
    "print(f\"TransE training parameters epochs={num_epochs}, batch_size={batch_size}\")\n",
    "_ = training_kg_loop.train(\n",
    "    triples_factory=reference_factory,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    use_tqdm=True,\n",
    ")\n",
    "\n",
    "print(\"TransE model trained with reference_kg.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding entity and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_to_id = reference_factory.entity_to_id\n",
    "relation_to_id = reference_factory.relation_to_id\n",
    "\n",
    "entity_representation = model.entity_representations[0]\n",
    "relation_representation = model.relation_representations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Train and Test data\n",
    "\n",
    "Extracts training and test data from RDF graphs using SPARQL queries. \n",
    "Training data includes triples and their truth values for supervised learning, while test data includes triples and their corresponding statement IRIs for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib.plugins.sparql import prepareQuery\n",
    "import rdflib\n",
    "\n",
    "# Train Data\n",
    "query_train = prepareQuery(\"\"\"\n",
    "    SELECT ?stmt ?subject ?predicate ?object ?truthValue\n",
    "    WHERE {\n",
    "        ?stmt a <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject> ?subject .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate> ?predicate .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#object> ?object .\n",
    "        ?stmt <http://swc2017.aksw.org/hasTruthValue> ?truthValue .\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "train_triples = []\n",
    "train_truthValue = []\n",
    "for fact_iri, sub, pred, obj, truth_value in train_graph.query(query_train):\n",
    "    train_triples.append((sub.toPython(),pred.toPython(),obj.toPython()))\n",
    "    train_truthValue.append(truth_value.toPython())\n",
    "\n",
    "# Test Data\n",
    "query_test = prepareQuery(\"\"\"\n",
    "    SELECT ?stmt ?subject ?predicate ?object\n",
    "    WHERE {\n",
    "        ?stmt a <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject> ?subject .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate> ?predicate .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#object> ?object .\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "test_triples = []\n",
    "test_fact_iri = []\n",
    "for fact_iri, sub, pred, obj in test_graph.query(query_test):\n",
    "    test_triples.append((sub.toPython(),pred.toPython(),obj.toPython()))\n",
    "    test_fact_iri.append(fact_iri.toPython())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoded on embedded model\n",
    "\n",
    "Function `get_embedding_for_fact` to retrieve the embeddings for a given fact (subject, predicate, object) using a trained TransE model.\n",
    "It then generates embeddings for all training and test triples. The training embeddings (X_train) are paired with their corresponding truth values (y_train), while the test embeddings (X_test) are prepared for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_for_fact(subj, pred, obj):\n",
    "    if subj not in entity_to_id or obj not in entity_to_id or pred not in relation_to_id:\n",
    "        emb_dim = model.entity_representations[0]._embeddings.weight.shape[-1]\n",
    "        return np.zeros(3 * emb_dim)\n",
    "\n",
    "    s_id = entity_to_id[subj]\n",
    "    p_id = relation_to_id[pred]\n",
    "    o_id = entity_to_id[obj]\n",
    "\n",
    "    s_emb = model.entity_representations[0](indices=torch.tensor([s_id]))  # shape [1, dim]\n",
    "    p_emb = model.relation_representations[0](indices=torch.tensor([p_id]))\n",
    "    o_emb = model.entity_representations[0](indices=torch.tensor([o_id]))\n",
    "\n",
    "    cat = torch.cat([s_emb[0], p_emb[0], o_emb[0]], dim=0)\n",
    "    return cat.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "X_train = [get_embedding_for_fact(s, p, o) for (s, p, o) in train_triples]\n",
    "y_train = np.array(train_truthValue)\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_test = [get_embedding_for_fact(s, p, o) for (s, p, o) in test_triples]\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train a MLPClassifier Model\n",
    "\n",
    "This code initializes and trains a Multi-Layer Perceptron (MLP) classifier with a specific architecture (three hidden layers with 256, 256, and 128 units), using the ReLU activation function and the Adam solver. It trains the model on the fact embeddings (X_train) and their corresponding truth values (y_train).\n",
    "The AUC (Area Under the ROC Curve) score is computed on the training data to evaluate model performance.\n",
    "The code also prepares the model to predict probabilities for the test data (X_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(256, 256, 128), activation=\"relu\", solver=\"adam\", max_iter=50, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "train_probs = mlp.predict_proba(X_train)[:, 1]\n",
    "train_auc = roc_auc_score(y_train, train_probs)\n",
    "print(f\"Train AUC: {train_auc:.4f}\")\n",
    "\n",
    "test_probs = mlp.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write it to result file\n",
    "\n",
    "Writes the test data results into a file (result.ttl).\n",
    "The lines have the following form:\n",
    "\n",
    "<http://dice-research.org/data/fb15k-237.ttl#3> <http://swc2017.aksw.org/hasTruthValue> \"5.11332036694511\"^^<http://www.w3.org/2001/XMLSchema#double> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result.ttl\", \"w\") as resultFile:\n",
    "    for fact_iri, score in zip(test_fact_iri, test_probs):\n",
    "        line = f'<{fact_iri}> <http://swc2017.aksw.org/hasTruthValue> \"{score}\"^^<http://www.w3.org/2001/XMLSchema#double> .\\n'\n",
    "        resultFile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
