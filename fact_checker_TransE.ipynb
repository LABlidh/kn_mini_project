{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact-Checking Engine\n",
    "\n",
    "## Test data path\n",
    "\n",
    "This constant holds the path to a test data file. Change this path accordingly. We recommend placing the test data file in the data folder within the unzipped directory and using a relative path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = \"data/fokg-sw-test-2024.nt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "Import the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py-310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph, URIRef, RDF, Namespace\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.models import TransE\n",
    "from pykeen.training import SLCWATrainingLoop\n",
    "from pykeen.losses import MarginRankingLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the reference, training, and test knowledge graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Knowledge Graph length. 675859\n",
      "Training data length. 5000\n",
      "Test data length. 2000\n"
     ]
    }
   ],
   "source": [
    "# Load reference knowledge graph\n",
    "reference_kg = Graph()\n",
    "reference_kg.parse(\"data/reference-kg.nt\", format=\"nt\")\n",
    "print(\"Reference Knowledge Graph length.\", len(reference_kg))\n",
    "\n",
    "# Load training data\n",
    "train_graph = Graph()\n",
    "train_graph.parse(\"data/fokg-sw-train-2024.nt\", format=\"nt\")\n",
    "print(\"Training data length.\", len(train_graph))\n",
    "\n",
    "#Load test data\n",
    "test_graph = Graph()\n",
    "test_graph.parse(TEST_DATA_PATH, format=\"nt\")\n",
    "print(\"Test data length.\", len(test_graph))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training TransE model with reference_kg\n",
    "\n",
    "Prepare the reference triples from the reference knowledge graph. Train the TransE model to learn embeddings for entities and relations in the reference knowledge graph, which can later be used for downstream tasks like link prediction or fact-checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of triples in reference_kg:  660000\n"
     ]
    }
   ],
   "source": [
    "reference_triples = [(str(subj), str(pred), str(obj)) for subj, pred, obj in reference_kg if isinstance(subj, rdflib.URIRef) and isinstance(obj, rdflib.URIRef)]\n",
    "print(\"Number of triples in reference_kg: \", len(reference_triples))\n",
    "reference_factory = TriplesFactory.from_labeled_triples(np.array(reference_triples, dtype=object))\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Train the TransE model on the reference knowledge graph.\n",
    "    \"\"\"\n",
    "    # TransE Model\n",
    "    embedding_dim = 200\n",
    "    margin = 1.0\n",
    "\n",
    "    model = TransE(\n",
    "        triples_factory=reference_factory,\n",
    "        embedding_dim=embedding_dim,\n",
    "        scoring_fct_norm=1,  # L1 distance\n",
    "        loss=MarginRankingLoss(margin=margin),  # margin-based ranking\n",
    "        random_seed= 42,\n",
    "    )\n",
    "\n",
    "    training_kg_loop = SLCWATrainingLoop(\n",
    "        model=model,\n",
    "        triples_factory=reference_factory,\n",
    "        optimizer=\"adam\",\n",
    "        optimizer_kwargs={\"lr\": 1e-3},\n",
    "        negative_sampler=\"basic\",\n",
    "        negative_sampler_kwargs={\"num_negs_per_pos\": 10},\n",
    "    )\n",
    "\n",
    "    num_epochs = 10\n",
    "    batch_size = 256\n",
    "    print(f\"TransE training parameters epochs={num_epochs}, batch_size={batch_size}\")\n",
    "    _ = training_kg_loop.train(\n",
    "        triples_factory=reference_factory,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        use_tqdm=True,\n",
    "    )\n",
    "\n",
    "    print(\"TransE model trained with reference_kg.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a Pretrained Model or Train a New One\n",
    "\n",
    "1. Faster Computation with Pretrained Model:\n",
    "If you want faster computation, the script will use the pretrained model (transE_model.pkl) if it exists in the same directory.\n",
    "\n",
    "2. Train the Model Yourself\n",
    "If you want to train the model from scratch, delete the \"transE_model.pkl\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransE training parameters epochs=10, batch_size=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs on cpu:   0%|          | 0/10 [01:39<?, ?epoch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already exists. Skipping save.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Save the trained model using pickle\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransE training parameters epochs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_kg_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtriples_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransE model trained with reference_kg.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-310/lib/python3.10/site-packages/pykeen/training/training_loop.py:394\u001b[0m, in \u001b[0;36mTrainingLoop.train\u001b[0;34m(self, triples_factory, num_epochs, batch_size, slice_size, label_smoothing, sampler, continue_training, only_size_probing, use_tqdm, use_tqdm_batch, tqdm_kwargs, stopper, sub_batch_size, num_workers, clear_optimizer, checkpoint_directory, checkpoint_name, checkpoint_frequency, checkpoint_on_failure, drop_last, callbacks, callbacks_kwargs, gradient_clipping_max_norm, gradient_clipping_norm_type, gradient_clipping_max_abs_value, pin_memory)\u001b[0m\n\u001b[1;32m    391\u001b[0m             temporary_directory \u001b[38;5;241m=\u001b[39m exit_stack\u001b[38;5;241m.\u001b[39menter_context(TemporaryDirectory())\n\u001b[1;32m    392\u001b[0m             best_epoch_model_file_path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(temporary_directory)\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 394\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mslice_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslice_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontinue_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinue_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[43monly_size_probing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_size_probing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_tqdm_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstopper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m            \u001b[49m\u001b[43msub_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msub_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_checkpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_on_failure_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_on_failure_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbest_epoch_model_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_epoch_model_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlast_best_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_best_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgradient_clipping_max_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_clipping_max_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgradient_clipping_norm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_clipping_norm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgradient_clipping_max_abs_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_clipping_max_abs_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtriples_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtriples_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# Ensure the release of memory\u001b[39;00m\n\u001b[1;32m    425\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-310/lib/python3.10/site-packages/pykeen/training/training_loop.py:727\u001b[0m, in \u001b[0;36mTrainingLoop._train\u001b[0;34m(self, triples_factory, num_epochs, batch_size, slice_size, label_smoothing, sampler, continue_training, only_size_probing, use_tqdm, use_tqdm_batch, tqdm_kwargs, stopper, sub_batch_size, num_workers, save_checkpoints, checkpoint_path, checkpoint_frequency, checkpoint_on_failure_file_path, best_epoch_model_file_path, last_best_epoch, drop_last, callbacks, callbacks_kwargs, gradient_clipping_max_norm, gradient_clipping_norm_type, gradient_clipping_max_abs_value, pin_memory)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m     batches \u001b[38;5;241m=\u001b[39m train_data_loader\n\u001b[0;32m--> 727\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43msub_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msub_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mslice_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslice_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43monly_size_probing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_size_probing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# When size probing we don't need the losses\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m only_size_probing:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-310/lib/python3.10/site-packages/pykeen/training/training_loop.py:487\u001b[0m, in \u001b[0;36mTrainingLoop._train_epoch\u001b[0;34m(self, batches, callbacks, sub_batch_size, label_smoothing, slice_size, epoch, only_size_probing, backward)\u001b[0m\n\u001b[1;32m    484\u001b[0m stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start \u001b[38;5;241m+\u001b[39m _sub_batch_size, current_batch_size)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# forward pass call\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mslice_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m current_epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n\u001b[1;32m    497\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_batch(epoch\u001b[38;5;241m=\u001b[39mepoch, batch\u001b[38;5;241m=\u001b[39mbatch, batch_loss\u001b[38;5;241m=\u001b[39mbatch_loss)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-310/lib/python3.10/site-packages/pykeen/training/training_loop.py:880\u001b[0m, in \u001b[0;36mTrainingLoop._forward_pass\u001b[0;34m(self, batch, start, stop, current_batch_size, label_smoothing, slice_size, backward)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backward:\n\u001b[0;32m--> 880\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m current_epoch_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpost_forward_pass()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-310/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-310/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-310/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the file path\n",
    "model_path = \"transE_model.pkl\"\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"File '{model_path}' already exists. Skipping save.\")\n",
    "else:\n",
    "    model = train_model()\n",
    "    # Save the trained model using pickle\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding entity and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_to_id = reference_factory.entity_to_id\n",
    "relation_to_id = reference_factory.relation_to_id\n",
    "\n",
    "entity_representation = model.entity_representations[0]\n",
    "relation_representation = model.relation_representations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Train and Test data\n",
    "\n",
    "Extracts training and test data from RDF graphs using SPARQL queries. \n",
    "Training data includes triples and their truth values for supervised learning, while test data includes triples and their corresponding statement IRIs for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data\n",
    "query_train = prepareQuery(\"\"\"\n",
    "    SELECT ?stmt ?subject ?predicate ?object ?truthValue\n",
    "    WHERE {\n",
    "        ?stmt a <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject> ?subject .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate> ?predicate .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#object> ?object .\n",
    "        ?stmt <http://swc2017.aksw.org/hasTruthValue> ?truthValue .\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "train_triples = []\n",
    "train_truthValue = []\n",
    "for fact_iri, sub, pred, obj, truth_value in train_graph.query(query_train):\n",
    "    train_triples.append((sub.toPython(),pred.toPython(),obj.toPython()))\n",
    "    train_truthValue.append(truth_value.toPython())\n",
    "\n",
    "# Test Data\n",
    "query_test = prepareQuery(\"\"\"\n",
    "    SELECT ?stmt ?subject ?predicate ?object\n",
    "    WHERE {\n",
    "        ?stmt a <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject> ?subject .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate> ?predicate .\n",
    "        ?stmt <http://www.w3.org/1999/02/22-rdf-syntax-ns#object> ?object .\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "test_triples = []\n",
    "test_fact_iri = []\n",
    "for fact_iri, sub, pred, obj in test_graph.query(query_test):\n",
    "    test_triples.append((sub.toPython(),pred.toPython(),obj.toPython()))\n",
    "    test_fact_iri.append(fact_iri.toPython())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoded on embedded model\n",
    "\n",
    "Function `get_embedding_for_fact` to retrieve the embeddings for a given fact (subject, predicate, object) using a trained TransE model.\n",
    "It then generates embeddings for all training and test triples. The training embeddings (X_train) are paired with their corresponding truth values (y_train), while the test embeddings (X_test) are prepared for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_for_fact(subj, pred, obj):\n",
    "    if subj not in entity_to_id or obj not in entity_to_id or pred not in relation_to_id:\n",
    "        emb_dim = model.entity_representations[0]._embeddings.weight.shape[-1]\n",
    "        return np.zeros(3 * emb_dim)\n",
    "\n",
    "    s_id = entity_to_id[subj]\n",
    "    p_id = relation_to_id[pred]\n",
    "    o_id = entity_to_id[obj]\n",
    "\n",
    "    s_emb = model.entity_representations[0](indices=torch.tensor([s_id]))  # shape [1, dim]\n",
    "    p_emb = model.relation_representations[0](indices=torch.tensor([p_id]))\n",
    "    o_emb = model.entity_representations[0](indices=torch.tensor([o_id]))\n",
    "\n",
    "    cat = torch.cat([s_emb[0], p_emb[0], o_emb[0]], dim=0)\n",
    "    return cat.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "X_train = [get_embeddings_for_fact(s, p, o) for (s, p, o) in train_triples]\n",
    "y_train = np.array(train_truthValue)\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_test = [get_embeddings_for_fact(s, p, o) for (s, p, o) in test_triples]\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train a MLPClassifier Model\n",
    "\n",
    "This code initializes and trains a Multi-Layer Perceptron (MLP) classifier with a specific architecture (three hidden layers with 256, 256, and 128 units), using the ReLU activation function and the Adam solver. It trains the model on the fact embeddings (X_train) and their corresponding truth values (y_train).\n",
    "The AUC (Area Under the ROC Curve) score is computed on the training data to evaluate model performance.\n",
    "The code also prepares the model to predict probabilities for the test data (X_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(256, 256, 128), activation=\"relu\", solver=\"adam\", max_iter=50, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "train_probs = mlp.predict_proba(X_train)[:, 1]\n",
    "train_auc = roc_auc_score(y_train, train_probs)\n",
    "print(f\"Train AUC: {train_auc:.4f}\")\n",
    "\n",
    "test_probs = mlp.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write it to result file\n",
    "\n",
    "Writes the test data results into a file (result.ttl).\n",
    "The lines have the following form:\n",
    "\n",
    "<http://dice-research.org/data/fb15k-237.ttl#3> <http://swc2017.aksw.org/hasTruthValue> \"5.11332036694511\"^^<http://www.w3.org/2001/XMLSchema#double> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result.ttl\", \"w\") as resultFile:\n",
    "    for fact_iri, score in zip(test_fact_iri, test_probs):\n",
    "        line = f'<{fact_iri}> <http://swc2017.aksw.org/hasTruthValue> \"{score}\"^^<http://www.w3.org/2001/XMLSchema#double> .\\n'\n",
    "        resultFile.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
